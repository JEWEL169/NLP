{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIiYO7RPr2VK",
        "outputId": "f5f8f6db-65fc-4348-d4cd-13c1b3d36b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag, ne_chunk, word_tokenize\n",
        "import spacy\n",
        "from nltk.tree import Tree\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('tagsets')\n",
        "\n",
        "words = ['bear', 'set', 'square', 'lead', 'criteria']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Find the 3rd meaning of the word in the list\n",
        "for word in words:\n",
        "    synsets = wn.synsets(word)\n",
        "    if len(synsets) >= 3:\n",
        "        print(f\"3rd Meaning of '{word}': {synsets[2].definition()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I89AMpvPsbux",
        "outputId": "e6a48b8c-966c-44d0-e0b6-93a21db9f3f3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3rd Meaning of 'bear': have\n",
            "3rd Meaning of 'set': several exercises intended to be done in series\n",
            "3rd Meaning of 'square': an open area at the meeting of two or more streets\n",
            "3rd Meaning of 'lead': evidence pointing to a possible solution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Extract the nouns of the word from the synonyms list\n",
        "for word in words:\n",
        "    nouns = [syn.name().split('.')[0] for syn in wn.synsets(word) if syn.pos() == 'n']\n",
        "    print(f\"Nouns for '{word}': {nouns}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WKqbEh6scWm",
        "outputId": "c296f313-fbd3-4e3b-d418-423712416647"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nouns for 'bear': ['bear', 'bear']\n",
            "Nouns for 'set': ['set', 'set', 'set', 'stage_set', 'set', 'bent', 'set', 'set', 'hardening', 'set', 'set', 'set', 'set']\n",
            "Nouns for 'square': ['square', 'square', 'public_square', 'square', 'square', 'square', 'square', 'square']\n",
            "Nouns for 'lead': ['lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'lead', 'star', 'lead', 'tip', 'lead', 'spark_advance', 'leash', 'lead', 'lead', 'jumper_cable', 'lead']\n",
            "Nouns for 'criteria': ['standard', 'criterion']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Extract the verbs of the word from the synonyms list\n",
        "for word in words:\n",
        "    verbs = [syn.name().split('.')[0] for syn in wn.synsets(word) if syn.pos() == 'v']\n",
        "    print(f\"Verbs for '{word}': {verbs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEwGU2VSsggP",
        "outputId": "93f26e9a-af30-4924-f38b-1ca12ca325fc"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verbs for 'bear': ['bear', 'give_birth', 'digest', 'bear', 'bear', 'bear', 'hold', 'yield', 'wear', 'behave', 'bear', 'hold', 'have_a_bun_in_the_oven']\n",
            "Verbs for 'set': ['put', 'determine', 'specify', 'set', 'set', 'set', 'fix', 'set', 'set', 'set', 'arrange', 'plant', 'set', 'jell', 'typeset', 'set', 'set', 'set', 'sic', 'place', 'rig', 'set_up', 'adjust', 'fructify', 'dress']\n",
            "Verbs for 'square': ['square', 'square', 'square', 'square', 'square', 'square', 'feather', 'feather']\n",
            "Verbs for 'lead': ['lead', 'leave', 'lead', 'lead', 'lead', 'run', 'head', 'lead', 'contribute', 'conduct', 'go', 'precede', 'run', 'moderate']\n",
            "Verbs for 'criteria': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Extract the adjectives of the word from the synonyms list\n",
        "for word in words:\n",
        "    adjectives = [syn.name().split('.')[0] for syn in wn.synsets(word) if syn.pos() == 'a']\n",
        "    print(f\"Adjectives for '{word}': {adjectives}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxilqUFSsjgs",
        "outputId": "6fd18786-95d2-4a92-ba7f-7e4dbe80c3a0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjectives for 'bear': []\n",
            "Adjectives for 'set': []\n",
            "Adjectives for 'square': ['square', 'straight']\n",
            "Adjectives for 'lead': []\n",
            "Adjectives for 'criteria': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# e) Extract the adverbs of the word from the synonyms list\n",
        "for word in words:\n",
        "    adverbs = [syn.name().split('.')[0] for syn in wn.synsets(word) if syn.pos() == 'r']\n",
        "    print(f\"Adverbs for '{word}': {adverbs}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACJkQ6pZsmkg",
        "outputId": "db5fb0ac-e8f9-44c6-80c0-69657cf88ae8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adverbs for 'bear': []\n",
            "Adverbs for 'set': []\n",
            "Adverbs for 'square': ['squarely', 'squarely', 'squarely']\n",
            "Adverbs for 'lead': []\n",
            "Adverbs for 'criteria': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# f) Extract the definition of the word\n",
        "for word in words:\n",
        "    synsets = wn.synsets(word)\n",
        "    if synsets:\n",
        "        print(f\"Definition of '{word}': {synsets[0].definition()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6fbqfoLsq9g",
        "outputId": "deef6c7d-4b46-467b-c18f-b7b5018b755c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Definition of 'bear': massive plantigrade carnivorous or omnivorous mammals with long shaggy coats and strong claws\n",
            "Definition of 'set': a group of things of the same kind that belong together and are so used\n",
            "Definition of 'square': (geometry) a plane rectangle with four equal sides and four right angles; a four-sided regular polygon\n",
            "Definition of 'lead': an advantage held by a competitor in a race\n",
            "Definition of 'criteria': a basis for comparison; a reference point against which other things can be evaluated\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# g) Program to get antonyms from WordNet\n",
        "for word in words:\n",
        "    antonyms = []\n",
        "    for syn in wn.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            if lemma.antonyms():\n",
        "                antonyms.append(lemma.antonyms()[0].name())\n",
        "    print(f\"Antonyms for '{word}': {antonyms}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykfK9XUaste-",
        "outputId": "08dee396-70a7-4617-9c0b-7e9d4ad83995"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Antonyms for 'bear': ['bull']\n",
            "Antonyms for 'set': ['rise']\n",
            "Antonyms for 'square': ['round', 'crooked']\n",
            "Antonyms for 'lead': ['deficit', 'follow']\n",
            "Antonyms for 'criteria': []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# h) Lemmatizing words using WordNet and comparison with stemming\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmed_words = {}\n",
        "\n",
        "for word in words:\n",
        "    lemma = lemmatizer.lemmatize(word)\n",
        "    stemmed_words[word] = lemma\n",
        "print(\"Lemmatized Words:\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq-cclOBszgf",
        "outputId": "bb319bfc-6671-4e3c-a118-19fabb1be79b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Words: {'bear': 'bear', 'set': 'set', 'square': 'square', 'lead': 'lead', 'criteria': 'criterion'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# i) Differentiating stemming and lemmatizing\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "print(\"Stemming and Lemmatization Comparison\")\n",
        "for word in words:\n",
        "    print(f\"Word: {word}, Porter Stem: {porter.stem(word)}, Lancaster Stem: {lancaster.stem(word)}, Lemma: {lemmatizer.lemmatize(word)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB_YGDdqs3c1",
        "outputId": "8898f547-9078-447c-e12e-271c0cb4d287"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming and Lemmatization Comparison\n",
            "Word: bear, Porter Stem: bear, Lancaster Stem: bear, Lemma: bear\n",
            "Word: set, Porter Stem: set, Lancaster Stem: set, Lemma: set\n",
            "Word: square, Porter Stem: squar, Lancaster Stem: squ, Lemma: square\n",
            "Word: lead, Porter Stem: lead, Lancaster Stem: lead, Lemma: lead\n",
            "Word: criteria, Porter Stem: criteria, Lancaster Stem: criter, Lemma: criterion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "# Download the required data package\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "print(\"Tokens:\", words)\n",
        "\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "def pos_tagging(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    print(\"Tokens:\", words)  # Debugging step\n",
        "    tagged = pos_tag(words)\n",
        "    print(\"PoS Tagging:\", tagged)\n",
        "\n",
        "# Test it\n",
        "sentence = \"John Doe visited Paris last summer\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0THZgC4XyUbh",
        "outputId": "9f8e6e15-168d-4f83-be43-45cad075470d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['John', 'Doe', 'visited', 'Paris', 'last', 'summer']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# k) Named Entity Recognition\n",
        "def named_entity_recognition(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    for ent in doc.ents:\n",
        "        print(f\"Entity: {ent.text}, Type: {ent.label_}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Named Entity Recognition ---\")\n",
        "named_entity_recognition(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucURZNcnyBEM",
        "outputId": "c1701b19-b94f-4d0a-a388-c3401f771233"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Named Entity Recognition ---\n",
            "Entity: John Doe, Type: PERSON\n",
            "Entity: Paris, Type: GPE\n",
            "Entity: last summer, Type: DATE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# l) Dependency and Constituency Parsing using spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"John Doe visited Paris last summer.\")\n",
        "\n",
        "print(\"\\nDependency Parsing:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.dep_} ({token.head.text})\")\n",
        "\n",
        "print(\"\\nConstituency Parsing (Subtrees):\")\n",
        "for sent in doc.sents:\n",
        "    print(list(sent.subtree))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuBYLpWItZHa",
        "outputId": "a368467b-559e-46c8-e509-7be1af3fc55f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dependency Parsing:\n",
            "John -> compound (Doe)\n",
            "Doe -> nsubj (visited)\n",
            "visited -> ROOT (visited)\n",
            "Paris -> dobj (visited)\n",
            "last -> amod (summer)\n",
            "summer -> npadvmod (visited)\n",
            ". -> punct (visited)\n",
            "\n",
            "Constituency Parsing (Subtrees):\n",
            "[John, Doe, visited, Paris, last, summer, .]\n"
          ]
        }
      ]
    }
  ]
}