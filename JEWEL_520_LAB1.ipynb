{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import (word_tokenize, sent_tokenize, RegexpTokenizer, TweetTokenizer, MWETokenizer, TreebankWordTokenizer)\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.utils import tokenize\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ],
      "metadata": {
        "id": "fTqb5U4w0HaA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "import os\n",
        "os.system(\"pip install spacy gensim tensorflow\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j8PM08O0LIY",
        "outputId": "bf3fd5eb-30fb-4154-a22f-9e0c4f6143ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paragraph with special characters, emojis, negations, and punctuations\n",
        "text = (\"The environment üåçüíö doesn't deserve such ignorance! Natural disasters, \"\n",
        "        \"like floods, hurricanes, and wildfires, are becoming common‚Äîlet's act \"\n",
        "        \"NOW. Please don't delay action; it's time for change! üå±üå¶Ô∏è\")\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMd5rKuW0Rc0",
        "outputId": "9bc395e4-1ad9-4592-fad3-b9b3bcf8bd9b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " The environment üåçüíö doesn't deserve such ignorance! Natural disasters, like floods, hurricanes, and wildfires, are becoming common‚Äîlet's act NOW. Please don't delay action; it's time for change! üå±üå¶Ô∏è\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTokenizations:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlNOrepT0g-E",
        "outputId": "c3f2eda8-3d41-4885-a295-7b22362c1313"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenizations:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Word Tokenization\n",
        "\n",
        "Insight: This tokenization method breaks text into individual words while preserving punctuation as separate tokens.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Suitable for basic tasks such as counting word frequencies, text preprocessing for sentiment analysis, and feature extraction for classification tasks."
      ],
      "metadata": {
        "id": "jiLS6v_L3WG8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a. Word Tokenization\n",
        "print(\"\\nWord Tokenization (nltk.word_tokenize):\")\n",
        "print(word_tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5ZwS9bp0ltl",
        "outputId": "558a1968-cf29-4002-c0de-394507df448b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Tokenization (nltk.word_tokenize):\n",
            "['The', 'environment', 'üåçüíö', 'does', \"n't\", 'deserve', 'such', 'ignorance', '!', 'Natural', 'disasters', ',', 'like', 'floods', ',', 'hurricanes', ',', 'and', 'wildfires', ',', 'are', 'becoming', 'common‚Äîlet', \"'s\", 'act', 'NOW', '.', 'Please', 'do', \"n't\", 'delay', 'action', ';', 'it', \"'s\", 'time', 'for', 'change', '!', 'üå±üå¶Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sentence Tokenization (nltk.sent_tokenize)\n",
        "\n",
        "Insight: It splits text into meaningful sentences based on punctuation and grammar rules.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Useful for text summarization, document parsing, and chatbot responses where understanding sentence boundaries is crucial."
      ],
      "metadata": {
        "id": "vxIwCFMT3z0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# b. Sentence Tokenization\n",
        "print(\"\\nSentence Tokenization (nltk.sent_tokenize):\")\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk_hZFCv1bue",
        "outputId": "e391165e-7555-4728-966e-e9f4d6ce2e4e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization (nltk.sent_tokenize):\n",
            "[\"The environment üåçüíö doesn't deserve such ignorance!\", \"Natural disasters, like floods, hurricanes, and wildfires, are becoming common‚Äîlet's act NOW.\", \"Please don't delay action; it's time for change!\", 'üå±üå¶Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Punctuation-based Tokenizer (RegexpTokenizer)\n",
        "\n",
        "Insight: Removes punctuation and focuses on capturing clean words.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Ideal for clean text analysis when punctuation is not needed, such as spam filtering or keyword extraction.\n"
      ],
      "metadata": {
        "id": "1DAtI8gN4vj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# c. Punctuation-based Tokenization\n",
        "print(\"\\nPunctuation-based Tokenization (RegexpTokenizer):\")\n",
        "regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "print(regex_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBkSVDDs1f3e",
        "outputId": "eaf99b8b-3790-4231-da98-f2b811bea69e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation-based Tokenization (RegexpTokenizer):\n",
            "['The', 'environment', 'doesn', 't', 'deserve', 'such', 'ignorance', 'Natural', 'disasters', 'like', 'floods', 'hurricanes', 'and', 'wildfires', 'are', 'becoming', 'common', 'let', 's', 'act', 'NOW', 'Please', 'don', 't', 'delay', 'action', 'it', 's', 'time', 'for', 'change']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Treebank Word Tokenizer (nltk.TreebankWordTokenizer)\n",
        "\n",
        "Insight: Handles contractions and special cases like hyphenated words.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Suitable for formal text processing in fields like legal document analysis or news article parsing."
      ],
      "metadata": {
        "id": "_tDgqnat41ZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# d. Treebank Word Tokenizer\n",
        "print(\"\\nTreebank Word Tokenization:\")\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "print(treebank_tokenizer.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjLJUs2U1koK",
        "outputId": "de7c54c3-16fd-4712-891e-b37d991a5976"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Word Tokenization:\n",
            "['The', 'environment', 'üåçüíö', 'does', \"n't\", 'deserve', 'such', 'ignorance', '!', 'Natural', 'disasters', ',', 'like', 'floods', ',', 'hurricanes', ',', 'and', 'wildfires', ',', 'are', 'becoming', 'common‚Äîlet', \"'s\", 'act', 'NOW.', 'Please', 'do', \"n't\", 'delay', 'action', ';', 'it', \"'s\", 'time', 'for', 'change', '!', 'üå±üå¶Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Tweet Tokenizer (nltk.TweetTokenizer)\n",
        "\n",
        "Insight: Efficiently handles social media-specific text, including emojis, hashtags, and URLs.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Best suited for mining social media platforms like Twitter for sentiment analysis, trend detection, and brand monitoring."
      ],
      "metadata": {
        "id": "5xi_a3h846oR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# e. Tweet Tokenizer\n",
        "print(\"\\nTweet Tokenization (nltk.TweetTokenizer):\")\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "print(tweet_tokenizer.tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWpALGFk1tTY",
        "outputId": "d51b59a1-fb44-46f1-defc-d0d0748326ac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization (nltk.TweetTokenizer):\n",
            "['The', 'environment', 'üåç', 'üíö', \"doesn't\", 'deserve', 'such', 'ignorance', '!', 'Natural', 'disasters', ',', 'like', 'floods', ',', 'hurricanes', ',', 'and', 'wildfires', ',', 'are', 'becoming', 'common', '‚Äî', \"let's\", 'act', 'NOW', '.', 'Please', \"don't\", 'delay', 'action', ';', \"it's\", 'time', 'for', 'change', '!', 'üå±', 'üå¶', 'Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Multi-Word Expression (MWE) Tokenizer (nltk.MWETokenizer)\n",
        "\n",
        "Insight: Recognizes predefined multi-word expressions as single tokens.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Useful for domain-specific tasks like entity recognition, phrase detection, and financial market analysis."
      ],
      "metadata": {
        "id": "2fDYWe8C5ANB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# f. Multi-Word Expression Tokenizer\n",
        "print(\"\\nMulti-Word Expression Tokenization:\")\n",
        "mwe_tokenizer = MWETokenizer([(\"Natural\", \"disasters\"), (\"time\", \"for\")])\n",
        "print(mwe_tokenizer.tokenize(text.split()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtQ5qspL12qL",
        "outputId": "f8677881-308b-489d-f959-1203d8ddcf25"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Word Expression Tokenization:\n",
            "['The', 'environment', 'üåçüíö', \"doesn't\", 'deserve', 'such', 'ignorance!', 'Natural', 'disasters,', 'like', 'floods,', 'hurricanes,', 'and', 'wildfires,', 'are', 'becoming', \"common‚Äîlet's\", 'act', 'NOW.', 'Please', \"don't\", 'delay', 'action;', \"it's\", 'time_for', 'change!', 'üå±üå¶Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. TextBlob Word Tokenizer\n",
        "\n",
        "Insight: Simplifies tokenization while offering integration with additional NLP features like part-of-speech tagging.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Suitable for rapid prototyping of text classification and sentiment analysis projects.\n"
      ],
      "metadata": {
        "id": "tkVEJack5Kwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# g. TextBlob Word Tokenizer\n",
        "print(\"\\nTextBlob Tokenization:\")\n",
        "blob = TextBlob(text)\n",
        "print(blob.words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOKsaK0319eo",
        "outputId": "e2101fa7-9437-40d0-f0c9-c83c5ad87bd9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TextBlob Tokenization:\n",
            "['The', 'environment', 'üåçüíö', 'does', \"n't\", 'deserve', 'such', 'ignorance', 'Natural', 'disasters', 'like', 'floods', 'hurricanes', 'and', 'wildfires', 'are', 'becoming', 'common‚Äîlet', \"'s\", 'act', 'NOW', 'Please', 'do', \"n't\", 'delay', 'action', 'it', \"'s\", 'time', 'for', 'change', 'üå±üå¶Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. spaCy Tokenizer\n",
        "\n",
        "Insight: A highly optimized tokenizer for large-scale NLP tasks, capable of recognizing complex linguistic structures.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Ideal for creating NLP pipelines, such as named entity recognition, dependency parsing, and text classification."
      ],
      "metadata": {
        "id": "7hucHfSH5Pkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# h. spaCy Tokenizer\n",
        "print(\"\\nspaCy Tokenization:\")\n",
        "doc = nlp(text)\n",
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ptzHtTw2DYq",
        "outputId": "3d9d2968-312e-4cb1-87ce-552411b49bef"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "spaCy Tokenization:\n",
            "['The', 'environment', 'üåç', 'üíö', 'does', \"n't\", 'deserve', 'such', 'ignorance', '!', 'Natural', 'disasters', ',', 'like', 'floods', ',', 'hurricanes', ',', 'and', 'wildfires', ',', 'are', 'becoming', 'common', '‚Äî', 'let', \"'s\", 'act', 'NOW', '.', 'Please', 'do', \"n't\", 'delay', 'action', ';', 'it', \"'s\", 'time', 'for', 'change', '!', 'üå±', 'üå¶', 'Ô∏è']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Gensim Word Tokenizer\n",
        "\n",
        "Insight: Focuses on generating tokens for text input in topic modeling and word embedding tasks.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Best for training word embeddings and building models for topic classification and similarity detection."
      ],
      "metadata": {
        "id": "fSZBHtgz5Xlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i. Gensim Word Tokenizer\n",
        "print(\"\\nGensim Tokenization:\")\n",
        "print(list(tokenize(text)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFMOHhbt2JNY",
        "outputId": "5a5b799a-5875-46dc-f5e4-5c21802c78de"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gensim Tokenization:\n",
            "['The', 'environment', 'doesn', 't', 'deserve', 'such', 'ignorance', 'Natural', 'disasters', 'like', 'floods', 'hurricanes', 'and', 'wildfires', 'are', 'becoming', 'common', 'let', 's', 'act', 'NOW', 'Please', 'don', 't', 'delay', 'action', 'it', 's', 'time', 'for', 'change']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Keras Tokenization (text_to_word_sequence)\n",
        "\n",
        "Insight: Designed for preprocessing text in deep learning pipelines by converting text into word sequences.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Applications: Essential for preparing input text for neural networks, such as LSTMs or CNNs, for tasks like text generation and sentiment analysis."
      ],
      "metadata": {
        "id": "Hkrp4VJy5bo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# j. Tokenization with Keras\n",
        "print(\"\\nKeras Tokenization:\")\n",
        "print(text_to_word_sequence(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZHH-squ2Mks",
        "outputId": "b8ba4e58-cf33-4bd6-96a3-cfa0145971e8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Keras Tokenization:\n",
            "['the', 'environment', 'üåçüíö', \"doesn't\", 'deserve', 'such', 'ignorance', 'natural', 'disasters', 'like', 'floods', 'hurricanes', 'and', 'wildfires', 'are', 'becoming', \"common‚Äîlet's\", 'act', 'now', 'please', \"don't\", 'delay', 'action', \"it's\", 'time', 'for', 'change', 'üå±üå¶Ô∏è']\n"
          ]
        }
      ]
    }
  ]
}